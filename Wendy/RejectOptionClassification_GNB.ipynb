{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d80a116e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No module named 'tempeh': LawSchoolGPADataset will be unavailable. To install, run:\n",
      "pip install 'aif360[LawSchoolGPA]'\n",
      "WARNING:root:No module named 'tensorflow': AdversarialDebiasing will be unavailable. To install, run:\n",
      "pip install 'aif360[AdversarialDebiasing]'\n",
      "WARNING:root:No module named 'tensorflow': AdversarialDebiasing will be unavailable. To install, run:\n",
      "pip install 'aif360[AdversarialDebiasing]'\n",
      "WARNING:root:No module named 'fairlearn': ExponentiatedGradientReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n",
      "WARNING:root:No module named 'fairlearn': GridSearchReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n",
      "WARNING:root:No module named 'fairlearn': GridSearchReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "# Load all necessary packages\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from warnings import warn\n",
    "\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.datasets import GermanDataset\n",
    "from aif360.metrics import ClassificationMetric, BinaryLabelDatasetMetric\n",
    "from aif360.metrics.utils import compute_boolean_conditioning_vector\n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions\\\n",
    "        import load_preproc_data_adult, load_preproc_data_german, load_preproc_data_compas\n",
    "from aif360.algorithms.postprocessing.reject_option_classification\\\n",
    "        import RejectOptionClassification\n",
    "from common_utils_ import compute_metrics\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interactive, FloatSlider"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfbb7c7",
   "metadata": {},
   "source": [
    "Load dataset and specify options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c19faf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import dataset\n",
    "protected_attribute_used = 2 # 1, 2\n",
    "\n",
    "dataset_orig = GermanDataset()\n",
    "if protected_attribute_used == 1:\n",
    "    privileged_groups = [{'sex': 1}]\n",
    "    unprivileged_groups = [{'sex': 0}]\n",
    "    dataset_orig = load_preproc_data_german(['sex'])\n",
    "else:\n",
    "    privileged_groups = [{'age': 1}]\n",
    "    unprivileged_groups = [{'age': 0}]\n",
    "    dataset_orig = load_preproc_data_german(['age'])\n",
    "    \n",
    "# Metric used (should be one of allowed_metrics)\n",
    "metric_name = \"Statistical parity difference\"\n",
    "\n",
    "# Upper and lower bound on the fairness metric used\n",
    "metric_ub = 0.05\n",
    "metric_lb = -0.05\n",
    "        \n",
    "#random seed for calibrated equal odds prediction\n",
    "np.random.seed(1)\n",
    "\n",
    "# Verify metric name\n",
    "allowed_metrics = [\"Statistical parity difference\",\n",
    "                   \"Average odds difference\",\n",
    "                   \"Equal opportunity difference\"]\n",
    "if metric_name not in allowed_metrics:\n",
    "    raise ValueError(\"Metric name should be one of allowed metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ff4262",
   "metadata": {},
   "source": [
    "Split into train, test and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b20d0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataset and split into train and test\n",
    "dataset_orig_train, dataset_orig_vt = dataset_orig.split([0.7], shuffle=True)\n",
    "dataset_orig_valid, dataset_orig_test = dataset_orig_vt.split([0.5], shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dd3c8d",
   "metadata": {},
   "source": [
    "Clean up training data and display properties of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "160eb80b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Training Dataset shape"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(700, 11)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Favorable and unfavorable labels"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 2.0\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Protected attribute names"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age']\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Privileged and unprivileged protected attribute values"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1.])] [array([0.])]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Dataset feature names"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'sex', 'credit_history=Delay', 'credit_history=None/Paid', 'credit_history=Other', 'savings=500+', 'savings=<500', 'savings=Unknown/None', 'employment=1-4 years', 'employment=4+ years', 'employment=Unemployed']\n"
     ]
    }
   ],
   "source": [
    "# print out some labels, names, etc.\n",
    "display(Markdown(\"#### Training Dataset shape\"))\n",
    "print(dataset_orig_train.features.shape)\n",
    "display(Markdown(\"#### Favorable and unfavorable labels\"))\n",
    "print(dataset_orig_train.favorable_label, dataset_orig_train.unfavorable_label)\n",
    "display(Markdown(\"#### Protected attribute names\"))\n",
    "print(dataset_orig_train.protected_attribute_names)\n",
    "display(Markdown(\"#### Privileged and unprivileged protected attribute values\"))\n",
    "print(dataset_orig_train.privileged_protected_attributes, \n",
    "      dataset_orig_train.unprivileged_protected_attributes)\n",
    "display(Markdown(\"#### Dataset feature names\"))\n",
    "print(dataset_orig_train.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83efec95",
   "metadata": {},
   "source": [
    "Metric for original training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4f0dc9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Original training dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in mean outcomes between unprivileged and privileged groups = -0.119044\n"
     ]
    }
   ],
   "source": [
    "metric_orig_train = BinaryLabelDatasetMetric(dataset_orig_train, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "display(Markdown(\"#### Original training dataset\"))\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_train.mean_difference())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebfa83d",
   "metadata": {},
   "source": [
    "Train classifier on original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96a6fbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GaussianNB classifier and predictions\n",
    "scale_orig = StandardScaler()\n",
    "X_train = scale_orig.fit_transform(dataset_orig_train.features)\n",
    "y_train = dataset_orig_train.labels.ravel()\n",
    "\n",
    "lmod =  GaussianNB()\n",
    "lmod.fit(X_train, y_train)\n",
    "y_train_pred = lmod.predict(X_train)\n",
    "\n",
    "# positive class index\n",
    "pos_ind = np.where(lmod.classes_ == dataset_orig_train.favorable_label)[0][0]\n",
    "\n",
    "dataset_orig_train_pred = dataset_orig_train.copy(deepcopy=True)\n",
    "dataset_orig_train_pred.labels = y_train_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7105004f",
   "metadata": {},
   "source": [
    "Obtain scores for validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "485cccda",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_orig_valid_pred = dataset_orig_valid.copy(deepcopy=True)\n",
    "X_valid = scale_orig.transform(dataset_orig_valid_pred.features)\n",
    "y_valid = dataset_orig_valid_pred.labels\n",
    "dataset_orig_valid_pred.scores = lmod.predict_proba(X_valid)[:,pos_ind].reshape(-1,1)\n",
    "\n",
    "dataset_orig_test_pred = dataset_orig_test.copy(deepcopy=True)\n",
    "X_test = scale_orig.transform(dataset_orig_test_pred.features)\n",
    "y_test = dataset_orig_test_pred.labels\n",
    "dataset_orig_test_pred.scores = lmod.predict_proba(X_test)[:,pos_ind].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a8dacd",
   "metadata": {},
   "source": [
    "### Find the optimal parameters from the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e4a698",
   "metadata": {},
   "source": [
    "Best threshold for classification only (no fairness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "795cc087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best balanced accuracy (no fairness constraints) = 0.7021\n",
      "Optimal classification threshold (no fairness constraints) = 0.7425\n"
     ]
    }
   ],
   "source": [
    "num_thresh = 100\n",
    "ba_arr = np.zeros(num_thresh)\n",
    "class_thresh_arr = np.linspace(0.01, 0.99, num_thresh)\n",
    "for idx, class_thresh in enumerate(class_thresh_arr):\n",
    "    \n",
    "    fav_inds = dataset_orig_valid_pred.scores > class_thresh\n",
    "    dataset_orig_valid_pred.labels[fav_inds] = dataset_orig_valid_pred.favorable_label\n",
    "    dataset_orig_valid_pred.labels[~fav_inds] = dataset_orig_valid_pred.unfavorable_label\n",
    "    \n",
    "    classified_metric_orig_valid = ClassificationMetric(dataset_orig_valid,\n",
    "                                             dataset_orig_valid_pred, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "    \n",
    "    ba_arr[idx] = 0.5*(classified_metric_orig_valid.true_positive_rate()\\\n",
    "                       +classified_metric_orig_valid.true_negative_rate())\n",
    "\n",
    "best_ind = np.where(ba_arr == np.max(ba_arr))[0][0]\n",
    "best_class_thresh = class_thresh_arr[best_ind]\n",
    "\n",
    "print(\"Best balanced accuracy (no fairness constraints) = %.4f\" % np.max(ba_arr))\n",
    "print(\"Optimal classification threshold (no fairness constraints) = %.4f\" % best_class_thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36e17c1",
   "metadata": {},
   "source": [
    "Estimate optimal parameters for the ROC method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6ef6788",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ROC = RejectOptionClassification(unprivileged_groups=unprivileged_groups, \n",
    "                                 privileged_groups=privileged_groups, \n",
    "                                 low_class_thresh=0.01, high_class_thresh=0.99,\n",
    "                                  num_class_thresh=100, num_ROC_margin=50,\n",
    "                                  metric_name=metric_name,\n",
    "                                  metric_ub=metric_ub, metric_lb=metric_lb)\n",
    "ROC = ROC.fit(dataset_orig_valid, dataset_orig_valid_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5b9a9d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal classification threshold (with fairness constraints) = 0.5841\n",
      "Optimal ROC margin = 0.2376\n"
     ]
    }
   ],
   "source": [
    "print(\"Optimal classification threshold (with fairness constraints) = %.4f\" % ROC.classification_threshold)\n",
    "print(\"Optimal ROC margin = %.4f\" % ROC.ROC_margin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e99bd4",
   "metadata": {},
   "source": [
    "Predictions from Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df3c237b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Validation set"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "##### Raw predictions - No fairness constraints, only maximizing balanced accuracy"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy = 0.7021\n",
      "Statistical parity difference = -0.2400\n",
      "Disparate impact = 0.5000\n",
      "Average odds difference = -0.0881\n",
      "Equal opportunity difference = 0.0545\n",
      "Theil index = 0.4155\n"
     ]
    }
   ],
   "source": [
    "# Metrics for the test set\n",
    "fav_inds = dataset_orig_valid_pred.scores > best_class_thresh\n",
    "dataset_orig_valid_pred.labels[fav_inds] = dataset_orig_valid_pred.favorable_label\n",
    "dataset_orig_valid_pred.labels[~fav_inds] = dataset_orig_valid_pred.unfavorable_label\n",
    "\n",
    "display(Markdown(\"#### Validation set\"))\n",
    "display(Markdown(\"##### Raw predictions - No fairness constraints, only maximizing balanced accuracy\"))\n",
    "\n",
    "metric_valid_bef = compute_metrics(dataset_orig_valid, dataset_orig_valid_pred, \n",
    "                unprivileged_groups, privileged_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99661e1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Validation set"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "##### Transformed predictions - With fairness constraints"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy = 0.6425\n",
      "Statistical parity difference = -0.0320\n",
      "Disparate impact = 0.9259\n",
      "Average odds difference = 0.0922\n",
      "Equal opportunity difference = 0.2152\n",
      "Theil index = 0.4755\n"
     ]
    }
   ],
   "source": [
    "# Transform the validation set\n",
    "dataset_transf_valid_pred = ROC.predict(dataset_orig_valid_pred)\n",
    "\n",
    "display(Markdown(\"#### Validation set\"))\n",
    "display(Markdown(\"##### Transformed predictions - With fairness constraints\"))\n",
    "metric_valid_aft = compute_metrics(dataset_orig_valid, dataset_transf_valid_pred, \n",
    "                unprivileged_groups, privileged_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5072a430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing: Check if the metric optimized has not become worse\n",
    "assert np.abs(metric_valid_aft[metric_name]) <= np.abs(metric_valid_bef[metric_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2826b7a1",
   "metadata": {},
   "source": [
    "### Predictions from Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e5ba28e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Test set"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "##### Raw predictions - No fairness constraints, only maximizing balanced accuracy"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy = 0.6476\n",
      "Statistical parity difference = -0.2746\n",
      "Disparate impact = 0.4766\n",
      "Average odds difference = -0.2451\n",
      "Equal opportunity difference = -0.2759\n",
      "Theil index = 0.4005\n"
     ]
    }
   ],
   "source": [
    "# Metrics for the test set\n",
    "fav_inds = dataset_orig_test_pred.scores > best_class_thresh\n",
    "dataset_orig_test_pred.labels[fav_inds] = dataset_orig_test_pred.favorable_label\n",
    "dataset_orig_test_pred.labels[~fav_inds] = dataset_orig_test_pred.unfavorable_label\n",
    "\n",
    "display(Markdown(\"#### Test set\"))\n",
    "display(Markdown(\"##### Raw predictions - No fairness constraints, only maximizing balanced accuracy\"))\n",
    "\n",
    "metric_test_bef = compute_metrics(dataset_orig_test, dataset_orig_test_pred, \n",
    "                unprivileged_groups, privileged_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "627e6cc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Test set"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "##### Transformed predictions - With fairness constraints"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy = 0.6667\n",
      "Statistical parity difference = -0.1815\n",
      "Disparate impact = 0.6115\n",
      "Average odds difference = -0.1515\n",
      "Equal opportunity difference = -0.1743\n",
      "Theil index = 0.4236\n"
     ]
    }
   ],
   "source": [
    "# Metrics for the transformed test set\n",
    "dataset_transf_test_pred = ROC.predict(dataset_orig_test_pred)\n",
    "\n",
    "display(Markdown(\"#### Test set\"))\n",
    "display(Markdown(\"##### Transformed predictions - With fairness constraints\"))\n",
    "metric_test_aft = compute_metrics(dataset_orig_test, dataset_transf_test_pred, \n",
    "                unprivileged_groups, privileged_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8fbf762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Summary of Optimal Parameters"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### We show the optimal parameters for all combinations of metrics optimized, dataset, and protected attributes below"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Fairness Metric: Statistical parity difference, Accuracy Metric: Balanced accuracy"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model used: GNB\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Performance metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═══════════════╤════════════════╤════════════════╤═════════════════╤═════════════════╕\n",
      "│ Dataset       │   Age(Acc-Bef) │   Age(Acc-Aft) │   Age(Fair-Bef) │   Age(Fair-Aft) │\n",
      "╞═══════════════╪════════════════╪════════════════╪═════════════════╪═════════════════╡\n",
      "│ German(Valid) │         0.7021 │         0.6425 │         -0.0881 │          0.0922 │\n",
      "├───────────────┼────────────────┼────────────────┼─────────────────┼─────────────────┤\n",
      "│ German(Test)  │         0.6476 │         0.6667 │         -0.2451 │         -0.1515 │\n",
      "╘═══════════════╧════════════════╧════════════════╧═════════════════╧═════════════════╛\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Fairness Metric: Average odds difference, Accuracy Metric: Balanced accuracy"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Performance metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═══════════════╤════════════════╤════════════════╤═════════════════╤═════════════════╕\n",
      "│ Dataset       │   Age(Acc-Bef) │   Age(Acc-Aft) │   Age(Fair-Bef) │   Age(Fair-Aft) │\n",
      "╞═══════════════╪════════════════╪════════════════╪═════════════════╪═════════════════╡\n",
      "│ German(Valid) │         0.7021 │         0.6425 │         -0.2400 │         -0.0320 │\n",
      "├───────────────┼────────────────┼────────────────┼─────────────────┼─────────────────┤\n",
      "│ German(Test)  │         0.6476 │         0.6667 │         -0.2746 │         -0.1815 │\n",
      "╘═══════════════╧════════════════╧════════════════╧═════════════════╧═════════════════╛\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Fairness Metric: Equal opportunity difference, Accuracy Metric: Balanced accuracy"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Performance metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═══════════════╤════════════════╤════════════════╤═════════════════╤═════════════════╕\n",
      "│ Dataset       │   Age(Acc-Bef) │   Age(Acc-Aft) │   Age(Fair-Bef) │   Age(Fair-Aft) │\n",
      "╞═══════════════╪════════════════╪════════════════╪═════════════════╪═════════════════╡\n",
      "│ German(Valid) │         0.7021 │         0.6425 │          0.0545 │          0.2152 │\n",
      "├───────────────┼────────────────┼────────────────┼─────────────────┼─────────────────┤\n",
      "│ German(Test)  │         0.6476 │         0.6667 │         -0.2759 │         -0.1743 │\n",
      "╘═══════════════╧════════════════╧════════════════╧═════════════════╧═════════════════╛\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Optimal Parameters"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═══════════╤════════════════════════╤═══════════════════════════════════╤═══════════════════════════════╕\n",
      "│ Dataset   │   Age (Class. thresh.) │   Age (Class. thresh. - fairness) │   Age (ROC margin - fairness) │\n",
      "╞═══════════╪════════════════════════╪═══════════════════════════════════╪═══════════════════════════════╡\n",
      "│ German    │                 0.7425 │                            0.5841 │                        0.2376 │\n",
      "╘═══════════╧════════════════════════╧═══════════════════════════════════╧═══════════════════════════════╛\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "display(Markdown(\"## Summary of Optimal Parameters\"))\n",
    "display(Markdown(\"### We show the optimal parameters for all combinations of metrics optimized, dataset, and protected attributes below\"))\n",
    "display(Markdown(\"#### Fairness Metric: Statistical parity difference, Accuracy Metric: Balanced accuracy\"))\n",
    "performance_table = {\n",
    "    'Dataset': [\"German(Valid)\", \"German(Test)\"],\n",
    "    'Age(Acc-Bef)': [metric_valid_bef[\"Balanced accuracy\"],metric_test_bef[\"Balanced accuracy\"]],\n",
    "    'Age(Acc-Aft)' : [metric_valid_aft[\"Balanced accuracy\"],metric_test_aft[\"Balanced accuracy\"]],\n",
    "    'Age(Fair-Bef)' : [metric_valid_bef[\"Average odds difference\"], metric_test_bef[\"Average odds difference\"]],\n",
    "    'Age(Fair-Aft)' : [metric_valid_aft[\"Average odds difference\"],metric_test_aft[\"Average odds difference\"]]\n",
    "    \n",
    "}\n",
    "\n",
    "print(f'\\nModel used: GNB')\n",
    "display(Markdown(\"#### Performance metrics\"))\n",
    "print(tabulate(performance_table, headers='keys', tablefmt='fancy_grid', floatfmt=\".4f\"))\n",
    "display(Markdown(\"#### Fairness Metric: Average odds difference, Accuracy Metric: Balanced accuracy\"))\n",
    "performance_table = {\n",
    "    'Dataset': [\"German(Valid)\", \"German(Test)\"],\n",
    "    'Age(Acc-Bef)': [metric_valid_bef[\"Balanced accuracy\"],metric_test_bef[\"Balanced accuracy\"]],\n",
    "    'Age(Acc-Aft)' : [metric_valid_aft[\"Balanced accuracy\"],metric_test_aft[\"Balanced accuracy\"]],\n",
    "    'Age(Fair-Bef)' : [metric_valid_bef[\"Statistical parity difference\"], metric_test_bef[\"Statistical parity difference\"]],\n",
    "    'Age(Fair-Aft)' : [metric_valid_aft[\"Statistical parity difference\"],metric_test_aft[\"Statistical parity difference\"]]\n",
    "    \n",
    "}\n",
    "#print(f'\\nModel used: {model_used}')\n",
    "display(Markdown(\"#### Performance metrics\"))\n",
    "print(tabulate(performance_table, headers='keys', tablefmt='fancy_grid', floatfmt=\".4f\"))\n",
    "\n",
    "display(Markdown(\"#### Fairness Metric: Equal opportunity difference, Accuracy Metric: Balanced accuracy\"))\n",
    "performance_table = {\n",
    "    'Dataset': [\"German(Valid)\", \"German(Test)\"],\n",
    "    'Age(Acc-Bef)': [metric_valid_bef[\"Balanced accuracy\"],metric_test_bef[\"Balanced accuracy\"]],\n",
    "    'Age(Acc-Aft)' : [metric_valid_aft[\"Balanced accuracy\"],metric_test_aft[\"Balanced accuracy\"]],\n",
    "    'Age(Fair-Bef)' : [metric_valid_bef[\"Equal opportunity difference\"], metric_test_bef[\"Equal opportunity difference\"]],\n",
    "    'Age(Fair-Aft)' : [metric_valid_aft[\"Equal opportunity difference\"],metric_test_aft[\"Equal opportunity difference\"]]\n",
    "    \n",
    "}\n",
    "#print(f'\\nModel used: {model_used}')\n",
    "display(Markdown(\"#### Performance metrics\"))\n",
    "print(tabulate(performance_table, headers='keys', tablefmt='fancy_grid', floatfmt=\".4f\"))\n",
    "\n",
    "optimal_table = {\n",
    "    'Dataset' : [\"German\"],\n",
    "    'Age (Class. thresh.)' : [best_class_thresh],\n",
    "    'Age (Class. thresh. - fairness)': [ROC.classification_threshold],\n",
    "    'Age (ROC margin - fairness)' : [ROC.ROC_margin]\n",
    "}\n",
    "\n",
    "display(Markdown(\"#### Optimal Parameters\"))\n",
    "print(tabulate(optimal_table, headers='keys', tablefmt='fancy_grid', floatfmt=\".4f\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b8ff60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
