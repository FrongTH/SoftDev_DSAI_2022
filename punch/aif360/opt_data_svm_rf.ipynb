{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.datasets import GermanDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions import load_preproc_data_german\n",
    "\n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.distortion_functions import get_distortion_german\n",
    "from aif360.algorithms.preprocessing.optim_preproc import OptimPreproc\n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.opt_tools import OptTools\n",
    "\n",
    "from common_utils import compute_metrics, plot_fairness_impact\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Display and plot \n",
    "from IPython.display import Markdown, display\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the model\n",
    "model_used = \"SVC\"  # SVC, RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "protected = \"age\"\n",
    "dataset = load_preproc_data_german([protected])\n",
    "privileged_groups = [{'age': 1}]\n",
    "unprivilege_groups = [{'age': 0}]\n",
    "optim_options = {\n",
    "    \"distortion_fun\" : get_distortion_german,\n",
    "    \"epsilon\" : 0.1,\n",
    "    \"clist\" : [0.99, 1.99, 2.99],\n",
    "    \"dlist\" : [.1, 0.05, 0]\n",
    "}\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_train_set, og_vt_set = dataset.split([0.7], shuffle=True)\n",
    "og_valid_set, og_test_set = og_vt_set.split([0.5], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Training Dataset shape"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(700, 11)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Favorable and unfavorable labels"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 2.0\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Protected attribute names"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age']\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Privileged and unprivileged protected attribute values"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1.])] [array([0.])]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Dataset feature names"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'sex', 'credit_history=Delay', 'credit_history=None/Paid', 'credit_history=Other', 'savings=500+', 'savings=<500', 'savings=Unknown/None', 'employment=1-4 years', 'employment=4+ years', 'employment=Unemployed']\n"
     ]
    }
   ],
   "source": [
    "# print out some labels, names, etc.\n",
    "display(Markdown(\"#### Training Dataset shape\"))\n",
    "print(og_train_set.features.shape)\n",
    "display(Markdown(\"#### Favorable and unfavorable labels\"))\n",
    "print(og_train_set.favorable_label, og_train_set.unfavorable_label)\n",
    "display(Markdown(\"#### Protected attribute names\"))\n",
    "print(og_train_set.protected_attribute_names)\n",
    "display(Markdown(\"#### Privileged and unprivileged protected attribute values\"))\n",
    "print(og_train_set.privileged_protected_attributes,\n",
    "      og_train_set.unprivileged_protected_attributes)\n",
    "display(Markdown(\"#### Dataset feature names\"))\n",
    "print(og_train_set.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimized Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Privileged and unprivileged groups specified will not be used. The protected attributes are directly specified in the data preprocessing function. The current implementation automatically adjusts for discrimination across all groups. This can be changed by changing the optimization code.\n",
      "\n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 9 times so far.\n",
      "\n",
      "\n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 10 times so far.\n",
      "\n",
      "\n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 11 times so far.\n",
      "\n",
      "\n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 12 times so far.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Preprocessing: Objective converged to 0.000000\n"
     ]
    }
   ],
   "source": [
    "OP = OptimPreproc(OptTools, optim_options, unprivileged_groups= unprivilege_groups, privileged_groups= privileged_groups)\n",
    "\n",
    "OP = OP.fit(og_train_set)      # optimize training set\n",
    "\n",
    "# transform data and align features\n",
    "trans_train_set = OP.transform(og_train_set,transform_Y=True)\n",
    "trans_train_set = og_train_set.align_datasets(trans_train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original mean diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset mean difference: -0.1452\n"
     ]
    }
   ],
   "source": [
    "og_diff_mean = BinaryLabelDatasetMetric(og_train_set, privileged_groups=privileged_groups, unprivileged_groups=unprivilege_groups)\n",
    "og_diff_mean_value = og_diff_mean.mean_difference()\n",
    "print(\"Original Dataset mean difference: %.4f\" % og_diff_mean_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform mean diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed Dataset mean difference: -0.1186\n"
     ]
    }
   ],
   "source": [
    "trans_diff_mean = BinaryLabelDatasetMetric(trans_train_set, privileged_groups=privileged_groups, unprivileged_groups=unprivilege_groups)\n",
    "trans_diff_mean_value = trans_diff_mean.mean_difference()\n",
    "print(\"Transformed Dataset mean difference: %.4f\" % trans_diff_mean_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Testing \n",
    "assert np.abs(trans_diff_mean.mean_difference()) < np.abs(og_diff_mean.mean_difference())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and clean test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Testing Dataset shape"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 11)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Original test dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in mean outcomes between unprivileged and privileged groups = -0.196929\n"
     ]
    }
   ],
   "source": [
    "og_test_set = trans_train_set.align_datasets(og_test_set)\n",
    "display(Markdown(\"#### Testing Dataset shape\"))\n",
    "print(og_test_set.features.shape)\n",
    "\n",
    "og_test_diff = BinaryLabelDatasetMetric(og_test_set, unprivileged_groups=unprivilege_groups, privileged_groups=privileged_groups)\n",
    "\n",
    "display(Markdown(\"#### Original test dataset\"))\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % og_test_diff.mean_difference())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Transformed test dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in mean outcomes between unprivileged and privileged groups = -0.151762\n"
     ]
    }
   ],
   "source": [
    "trans_test_set = OP.transform(og_test_set, transform_Y = True)\n",
    "trans_test_set = og_test_set.align_datasets(trans_test_set)\n",
    "\n",
    "trans_test_diff = BinaryLabelDatasetMetric(trans_test_set, \n",
    "                                         unprivileged_groups=unprivilege_groups,\n",
    "                                         privileged_groups=privileged_groups)\n",
    "display(Markdown(\"#### Transformed test dataset\"))\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % trans_test_diff.mean_difference())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Original] Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_train = og_train_set.labels.ravel()\n",
    "\n",
    "if model_used == \"SVC\":\n",
    "    clf_model = LinearSVC()\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(og_train_set.features)\n",
    "if model_used == \"RF\":\n",
    "    clf_model = RandomForestClassifier()\n",
    "    X_train = og_train_set.features\n",
    "\n",
    "# ==================================================\n",
    "timer_str = time.time()\n",
    "mem_str = psutil.Process().memory_info().rss / (1024 * 1024)\n",
    "\n",
    "clf_model.fit(X_train, y_train)\n",
    "\n",
    "timer_stp = time.time()\n",
    "mem_stp = psutil.Process().memory_info().rss / (1024 * 1024)\n",
    "\n",
    "og_train_time = timer_stp - timer_str\n",
    "og_train_mem = mem_stp - mem_str\n",
    "\n",
    "# ==================================================\n",
    "# positive class index \n",
    "pos_ind = np.where(clf_model.classes_ == og_train_set.favorable_label)[0][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_valid_set_pred = og_valid_set.copy(deepcopy=True)\n",
    "og_test_pred = og_test_set.copy(deepcopy=True)\n",
    "\n",
    "y_valid = og_valid_set_pred.labels\n",
    "y_test = og_test_pred.labels\n",
    "\n",
    "if model_used == \"SVC\":\n",
    "    X_valid = scaler.fit_transform(og_valid_set_pred.features)\n",
    "    # predict on vaidation set\n",
    "    og_valid_set_pred.scores = clf_model._predict_proba_lr(X_valid)[:, pos_ind].reshape(-1,1)\n",
    "\n",
    "    X_test = scaler.fit_transform(og_test_pred.features)\n",
    "    # predict on test set\n",
    "    og_test_pred.scores = clf_model._predict_proba_lr(X_test)[:, pos_ind].reshape(-1,1)\n",
    "\n",
    "if model_used == \"RF\":\n",
    "    X_valid = og_valid_set_pred.features\n",
    "    og_valid_set_pred.scores = clf_model.predict_proba(X_valid)[:, pos_ind].reshape(-1,1)\n",
    "\n",
    "    X_test = og_test_pred.features\n",
    "    og_test_pred.scores = clf_model.predict_proba(X_test)[:, pos_ind].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimal Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best balanced accuracy (No reweighting) = 0.6837\n",
      "Best recall (no reweighting) = 1.0000\n",
      "Optimal threshold based on accuracy = 0.6237\n",
      "Optimal threshold based on recall = 0.6237 \n"
     ]
    }
   ],
   "source": [
    "# Find optimal threshold based on balanced accuracy\n",
    "num_thresh = 100    # 0.00 - 1.00\n",
    "ba_acc = np.zeros(num_thresh)   # balance accuracy for each threshold\n",
    "recall_acc = np.zeros(num_thresh)   # recall for each threshold\n",
    "class_thresh_arr = np.linspace(0.01, 0.99, num_thresh)  # search space \n",
    "\n",
    "for idx, threshold in enumerate(class_thresh_arr):\n",
    "    # set labels if probability > threshold \n",
    "    fav_inds = og_valid_set_pred.scores > threshold\n",
    "    og_valid_set_pred.labels[fav_inds] = og_valid_set_pred.favorable_label\n",
    "    og_valid_set_pred.labels[~fav_inds] = og_valid_set_pred.unfavorable_label\n",
    "\n",
    "    # classified metric \n",
    "    og_valid_class_met = ClassificationMetric(og_valid_set, og_valid_set_pred, privileged_groups=privileged_groups, unprivileged_groups=unprivilege_groups)\n",
    "\n",
    "    ba_acc[idx] = 0.5 * (og_valid_class_met.true_positive_rate() + og_valid_class_met.true_negative_rate())\n",
    "\n",
    "    recall_acc[idx] = og_valid_class_met.recall()\n",
    "\n",
    "best_idx_acc = np.where(ba_acc == np.max(ba_acc))[0][0]\n",
    "best_idx_recall = np.where(recall_acc == np.max(recall_acc))[0][0]\n",
    "\n",
    "best_class_thresh_acc = class_thresh_arr[best_idx_acc]\n",
    "best_class_thresh_recall = class_thresh_arr[best_idx_acc]\n",
    "\n",
    "print(\"Best balanced accuracy (No reweighting) = %.4f\" % np.max(ba_acc))\n",
    "print(\"Best recall (no reweighting) = %.4f\" % np.max(recall_acc))\n",
    "\n",
    "print(\"Optimal threshold based on accuracy = %.4f\" % best_class_thresh_acc)\n",
    "print(\"Optimal threshold based on recall = %.4f \" % best_class_thresh_recall)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Original] Predicton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Prediction from original testing data"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Testing set"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "##### Raw predictions - No fairness constraints"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theshold used = 0.6237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]invalid value encountered in double_scalars\n",
      "invalid value encountered in double_scalars\n",
      "\r100it [00:00, 1568.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy = 0.6066\n",
      "Precision = 0.8113\n",
      "Recall = 0.4216\n",
      "F1 = 0.5548\n",
      "Disparate impact = 0.0876\n",
      "Average odds difference = -0.3458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from dis import dis\n",
    "\n",
    "\n",
    "display(Markdown(\"#### Prediction from original testing data\"))\n",
    "# performance metrics\n",
    "og_bal_acc = []\n",
    "og_precision = []\n",
    "og_recall = []\n",
    "og_f1 = []\n",
    "\n",
    "# fairness metrics\n",
    "og_dis_imp = []\n",
    "og_avg_odds_diff = []\n",
    "\n",
    "thresh_idx = 0   # index of the best threshold\n",
    "\n",
    "display(Markdown(\"#### Testing set\"))\n",
    "display(Markdown(\"##### Raw predictions - No fairness constraints\"))\n",
    "\n",
    "print(\"Theshold used = %.4f\" % best_class_thresh_acc)\\\n",
    "\n",
    "for idx, thresh in tqdm(enumerate(class_thresh_arr)):\n",
    "    if thresh == best_class_thresh_acc:\n",
    "        disp = True\n",
    "        thresh_idx = idx\n",
    "    else:\n",
    "        disp = False\n",
    "\n",
    "    fav_inds = og_test_pred.scores > thresh\n",
    "    og_test_pred.labels[fav_inds] = og_test_pred.favorable_label\n",
    "    og_test_pred.labels[~fav_inds] = og_test_pred.unfavorable_label\n",
    "\n",
    "    og_test_class_met = compute_metrics(og_test_set, og_test_pred, unprivilege_groups, privileged_groups, disp=disp)\n",
    "\n",
    "    # performance\n",
    "    og_bal_acc.append(og_test_class_met[\"Balanced accuracy\"])\n",
    "    og_precision.append(og_test_class_met[\"Precision\"])\n",
    "    og_recall.append(og_test_class_met[\"Recall\"])\n",
    "    og_f1.append(og_test_class_met[\"F1\"])\n",
    "    \n",
    "    # fairness\n",
    "    og_dis_imp.append(og_test_class_met[\"Disparate impact\"])\n",
    "    og_avg_odds_diff.append(og_test_class_met[\"Average odds difference\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Transf] Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = trans_train_set.labels.ravel()\n",
    "\n",
    "if model_used == \"SVC\":\n",
    "    clf_model = LinearSVC()\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(trans_train_set.features)\n",
    "if model_used == \"RF\":\n",
    "    clf_model = RandomForestClassifier()\n",
    "    X_train = trans_train_set.features\n",
    "\n",
    "timer_str = time.time()\n",
    "mem_str = psutil.Process().memory_info().rss / (1024 * 1024)\n",
    "\n",
    "clf_model.fit(X_train, y_train, sample_weight=trans_train_set.instance_weights)\n",
    "\n",
    "timer_stp = time.time()\n",
    "mem_stp = psutil.Process().memory_info().rss / (1024 * 1024)\n",
    "\n",
    "trans_train_time = timer_stp - timer_str\n",
    "trans_train_mem = mem_stp - mem_str\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict on trans test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_test_pred = trans_test_set.copy(deepcopy=True)\n",
    "y_test = trans_test_pred.labels\n",
    "\n",
    "if model_used == \"SVC\":\n",
    "    X_test = scaler.fit_transform(trans_test_pred.features)\n",
    "    trans_test_pred.scores = clf_model._predict_proba_lr(X_test)[:, pos_ind].reshape(-1,1)\n",
    "if model_used == \"RF\":\n",
    "    X_test = trans_test_pred.features\n",
    "    trans_test_pred.scores = clf_model.predict_proba(X_test)[:, pos_ind].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Prediction from transformed data"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theshold used = 0.6237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy = 0.6066\n",
      "Precision = 0.8113\n",
      "Recall = 0.4216\n",
      "F1 = 0.5548\n",
      "Disparate impact = 0.2733\n",
      "Average odds difference = -0.2688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invalid value encountered in double_scalars\n",
      "invalid value encountered in double_scalars\n",
      "100it [00:00, 1731.75it/s]\n"
     ]
    }
   ],
   "source": [
    "display(Markdown(\"#### Prediction from transformed data\"))\n",
    "# performance metrics\n",
    "trans_bal_acc = []\n",
    "trans_precision = []\n",
    "trans_recall = []\n",
    "trans_f1 = []\n",
    "\n",
    "# fairness metrics\n",
    "trans_dis_imp = []\n",
    "trans_avg_odds_diff = []\n",
    "\n",
    "trans_thresh_idx = 0   # index of the best threshold\n",
    "\n",
    "print(\"Theshold used = %.4f\" % best_class_thresh_acc)\\\n",
    "\n",
    "for idx, thresh in tqdm(enumerate(class_thresh_arr)):\n",
    "    if thresh == best_class_thresh_acc:\n",
    "        disp = True\n",
    "        trans_thresh_idx = idx\n",
    "    else:\n",
    "        disp = False\n",
    "\n",
    "    fav_inds = trans_test_pred.scores > thresh\n",
    "    trans_test_pred.labels[fav_inds] = trans_test_pred.favorable_label\n",
    "    trans_test_pred.labels[~fav_inds] = trans_test_pred.unfavorable_label\n",
    "\n",
    "    trans_test_class_met = compute_metrics(\n",
    "        trans_test_set, trans_test_pred, unprivilege_groups, privileged_groups, disp=disp)\n",
    "\n",
    "    # performance\n",
    "    trans_bal_acc.append(trans_test_class_met[\"Balanced accuracy\"])\n",
    "    trans_precision.append(trans_test_class_met[\"Precision\"])\n",
    "    trans_recall.append(trans_test_class_met[\"Recall\"])\n",
    "    trans_f1.append(trans_test_class_met[\"F1\"])\n",
    "    \n",
    "    # fairness\n",
    "    trans_dis_imp.append(trans_test_class_met[\"Disparate impact\"])\n",
    "    trans_avg_odds_diff.append(trans_test_class_met[\"Average odds difference\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Summary"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model used: SVC\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Performance metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═══════════════╤════════════╤═════════════╤══════════╤════════╤════════╤══════════╕\n",
      "│ Reweighting   │   Accuracy │   Precision │   Recall │     F1 │   Time │   Memory │\n",
      "╞═══════════════╪════════════╪═════════════╪══════════╪════════╪════════╪══════════╡\n",
      "│ Before        │     0.6066 │      0.8113 │   0.4216 │ 0.5548 │ 0.0221 │   0.0117 │\n",
      "├───────────────┼────────────┼─────────────┼──────────┼────────┼────────┼──────────┤\n",
      "│ After         │     0.6066 │      0.8113 │   0.4216 │ 0.5548 │ 0.0277 │   0.0000 │\n",
      "╘═══════════════╧════════════╧═════════════╧══════════╧════════╧════════╧══════════╛\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Fairness metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒══════════════╤══════════╤════════════════════╤═══════════════════════════╕\n",
      "│ Reweighing   │   Recall │   Disparate Impact │   Average Odds Difference │\n",
      "╞══════════════╪══════════╪════════════════════╪═══════════════════════════╡\n",
      "│ Before       │   0.4216 │             0.0876 │                   -0.3458 │\n",
      "├──────────────┼──────────┼────────────────────┼───────────────────────────┤\n",
      "│ After        │   0.4216 │             0.2733 │                   -0.2688 │\n",
      "╘══════════════╧══════════╧════════════════════╧═══════════════════════════╛\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "##### Noted for the classification to be fair"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disparate impact: higher is better.\n",
      "Average odds difference: must be close to zero.\n"
     ]
    }
   ],
   "source": [
    "display(Markdown(\"## Summary\"))\n",
    "\n",
    "performance_table = {\n",
    "    'Reweighting': [\"Before\", \"After\"],\n",
    "    'Accuracy': [og_bal_acc[thresh_idx], trans_bal_acc[thresh_idx]],\n",
    "    'Precision' : [og_precision[thresh_idx], trans_precision[thresh_idx]],\n",
    "    'Recall' : [og_recall[thresh_idx], trans_recall[thresh_idx]],\n",
    "    'F1' : [og_f1[thresh_idx], trans_f1[thresh_idx]],\n",
    "    'Time' : [og_train_time, trans_train_time],\n",
    "    'Memory' : [og_train_mem, trans_train_mem]\n",
    "}\n",
    "\n",
    "print(f'\\nModel used: {model_used}')\n",
    "display(Markdown(\"#### Performance metrics\"))\n",
    "print(tabulate(performance_table, headers='keys', tablefmt='fancy_grid', floatfmt=\".4f\"))\n",
    "\n",
    "fairness_table = {\n",
    "    'Reweighing' : [\"Before\", \"After\"],\n",
    "    'Recall' : [og_recall[thresh_idx], trans_recall[thresh_idx]],\n",
    "    'Disparate Impact': [og_dis_imp[thresh_idx], trans_dis_imp[thresh_idx]],\n",
    "    'Average Odds Difference' : [og_avg_odds_diff[thresh_idx], trans_avg_odds_diff[thresh_idx]]\n",
    "}\n",
    "\n",
    "display(Markdown(\"#### Fairness metrics\"))\n",
    "print(tabulate(fairness_table, headers='keys', tablefmt='fancy_grid', floatfmt=\".4f\"))\n",
    "display(Markdown(\"##### Noted for the classification to be fair\"))\n",
    "print(\"Disparate impact: higher is better.\\nAverage odds difference: must be close to zero.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ff4f995c52911191224c4e02ed15d46d239801bb9bed64226067616b7558227c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
